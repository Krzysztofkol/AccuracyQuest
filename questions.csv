question|answer|user_answer
(The Elements of Statistical Learning). W SVM z jądrem RBF, gdy gamma dąży do nieskończoności, a C dąży do zera, model zawsze staje się równoważny klasyfikatorowi najbliższego sąsiada z głosowaniem ważonym odległością.|False|False
(The Elements of Statistical Learning). W SVM z liniowym jądrem, wektor wag w przestrzeni cech jest zawsze prostopadły do hiperpłaszczyzny rozdzielającej klasy w punkcie środkowym między najbliższymi punktami przeciwnych klas.|False|
(An Introduction to Statistical Learning). Warunek KKT (Karush-Kuhn-Tucker) mówiący, że α_i * (y_i(w^T x_i + b) - 1 + ξ_i) = 0, implikuje, że wszystkie wektory nośne leżą dokładnie na marginesie w przypadku miękkiego SVM.|False|
"(An Introduction to Statistical Learning). W SVM do klasyfikacji wieloklasowej metodą ""jeden-przeciwko-jednemu"", liczba trenowanych klasyfikatorów jest zawsze równa sumie liczb naturalnych od 1 do (n-1), gdzie n to liczba klas."|True|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z liniowym jądrem, wektor normalny hiperpłaszczyzny decyzyjnej jest zawsze ortogonalny do wszystkich wektorów nośnych.|False|
(The Elements of Statistical Learning). Metoda SMO (Sequential Minimal Optimization) w SVM gwarantuje, że w każdej iteracji suma mnożników Lagrange'a pozostaje stała.|True|
(An Introduction to Statistical Learning). W SVM z miękkim marginesem, zwiększenie parametru C zawsze prowadzi do zmniejszenia liczby wektorów nośnych.|False|
"(An Introduction to Statistical Learning). W SVM do klasyfikacji wieloklasowej metodą ""jeden-przeciwko-jednemu"", każdy punkt treningowy jest używany jako przykład pozytywny dokładnie w tylu klasyfikatorach binarnych, ile jest pozostałych klas."|True|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z jądrem RBF, gdy gamma dąży do nieskończoności, a C jest stałe, model zawsze staje się przeuczony, niezależnie od rozkładu danych.|False|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z miękkim marginesem, suma wszystkich zmiennych luzu (slack variables) jest zawsze mniejsza lub równa parametrowi C pomnożonemu przez liczbę przykładów treningowych.|True|
"(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM do klasyfikacji wieloklasowej metodą ""jeden-przeciwko-wszystkim"", liczba trenowanych klasyfikatorów jest zawsze równa liczbie klas minus jeden."|False|
(An Introduction to Statistical Learning). W SVM do regresji (SVR), punkty treningowe leżące na granicy ε-rury zawsze mają niezerowe mnożniki Lagrange'a.|True|
(An Introduction to Statistical Learning). W SVM z jądrem wielomianowym stopnia d, liczba cech w przestrzeni transformowanej jest zawsze równa liczbie permutacji z powtórzeniami d elementów z p elementów, gdzie p jest liczbą cech w oryginalnej przestrzeni.|False|
(An Introduction to Statistical Learning). W SVM z jądrem RBF, gdy gamma dąży do nieskończoności, a C jest stałe, model zawsze staje się równoważny klasyfikatorowi najbliższego sąsiada z odległością Manhattan.|False|
(The Elements of Statistical Learning). Metoda ν-SVM gwarantuje, że parametr ν jest górnym ograniczeniem frakcji błędów treningowych i dolnym ograniczeniem frakcji wektorów nośnych tylko dla pewnych wartości ν z przedziału (0,1).|False|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z jądrem wielomianowym, zwiększanie stopnia wielomianu zawsze prowadzi do zwiększenia liczby wektorów nośnych, ale nie zawsze do poprawy separacji klas w oryginalnej przestrzeni cech.|True|
(The Elements of Statistical Learning). Metoda ν-SVM gwarantuje, że parametr ν jest dokładną frakcją wektorów nośnych w zbiorze treningowym, niezależnie od rozkładu danych.|False|
(The Elements of Statistical Learning). W SVM z miękkim marginesem, punkty treningowe z αi = C są zawsze błędnie sklasyfikowane przez model.|False|
(An Introduction to Statistical Learning). W SVM do regresji (SVR), punkty treningowe leżące wewnątrz ε-rury zawsze mają dodatnie mnożniki Lagrange'a, ale mniejsze niż C.|False|
(The Elements of Statistical Learning). Metoda SMO (Sequential Minimal Optimization) w SVM gwarantuje, że w każdej iteracji co najmniej jeden mnożnik Lagrange'a zmieni swoją wartość.|False|
(The Elements of Statistical Learning). W SVM z jądrem RBF, gdy gamma dąży do nieskończoności, a C dąży do nieskończoności, model zawsze staje się równoważny klasyfikatorowi najbliższego sąsiada z odległością euklidesową.|True|
(An Introduction to Statistical Learning). W SVM z jądrem wielomianowym stopnia d, liczba cech w przestrzeni transformowanej jest zawsze równa d do potęgi liczby cech w oryginalnej przestrzeni plus jeden.|False|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z jądrem liniowym, zwiększanie parametru C zawsze prowadzi do zwiększenia liczby wektorów nośnych.|False|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z miękkim marginesem, punkty treningowe z 0 < αi < C zawsze leżą dokładnie na marginesie decyzyjnym i mają zerowe zmienne luzu (slack variables).|False|
(An Introduction to Statistical Learning). W SVM z jądrem RBF, gdy gamma dąży do zera, a C dąży do zera, model zawsze staje się równoważny klasyfikatorowi stałemu, który przypisuje wszystkim punktom średnią ważoną klas treningowych.|False|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z jądrem wielomianowym, zwiększanie stopnia wielomianu zawsze prowadzi do zmniejszenia błędu generalizacji, niezależnie od wartości parametru C.|False|
(An Introduction to Statistical Learning). W SVM z jądrem RBF, gdy gamma dąży do zera, a C dąży do nieskończoności, model zawsze staje się równoważny klasyfikatorowi liniowemu bez regularyzacji.|True|
(An Introduction to Statistical Learning). W SVM do regresji (SVR), punkty treningowe leżące dokładnie na granicy ε-rury zawsze mają mnożniki Lagrange'a równe połowie wartości parametru C.|False|
(An Introduction to Statistical Learning). W SVM z jądrem wielomianowym, zwiększanie stopnia wielomianu zawsze prowadzi do zmniejszenia marginesu funkcjonalnego w przestrzeni cech, niezależnie od wartości parametru C.|False|
(The Elements of Statistical Learning). W SVM z jądrem RBF, gdy gamma dąży do zera, a C jest stałe, model zawsze staje się równoważny klasyfikatorowi stałemu, który przypisuje wszystkim punktom klasę większościową.|True|
(The Elements of Statistical Learning). W SVM z jądrem RBF, gdy gamma dąży do nieskończoności, a C dąży do nieskończoności, model zawsze staje się równoważny klasyfikatorowi najbliższego sąsiada z odległością Mahalanobisa.|False|
"(An Introduction to Statistical Learning). W SVM do klasyfikacji wieloklasowej metodą ""jeden-przeciwko-jednemu"", każdy punkt treningowy jest używany jako przykład negatywny dokładnie w tylu klasyfikatorach binarnych, ile jest klas minus dwa."|True|
"(The Elements of Statistical Learning). W SVM do klasyfikacji wieloklasowej metodą ""jeden-przeciwko-jednemu"", liczba trenowanych klasyfikatorów jest zawsze równa n(n-1)/2, gdzie n to liczba klas."|True|
(The Elements of Statistical Learning). W SVM z jądrem gaussowskim, gdy gamma dąży do nieskończoności, klasyfikator SVM zawsze staje się równoważny klasyfikatorowi k-najbliższych sąsiadów z k=1.|False|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM do regresji (SVR), wszystkie punkty treningowe leżące dokładnie na granicy ε-rury mają niezerowe mnożniki Lagrange'a, ale mniejsze niż C.|True|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM do regresji (SVR), wszystkie punkty treningowe leżące poza ε-rurą mają zmienne luzu (slack variables) większe od zera, ale mniejsze od ε.|False|
(An Introduction to Statistical Learning). W SVM z jądrem wielomianowym, zwiększanie stopnia wielomianu zawsze prowadzi do zwiększenia efektywnej liczby wymiarów w przestrzeni cech, ale nie zawsze do poprawy separacji klas.|True|
(The Elements of Statistical Learning). W SVM z liniowym jądrem, wektor wag w przestrzeni cech jest zawsze ortogonalny do różnicy dowolnych dwóch wektorów nośnych z przeciwnych klas.|True|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z miękkim marginesem, punkty treningowe z αi = C zawsze leżą po niewłaściwej stronie marginesu lub wewnątrz niego.|True|
(The Elements of Statistical Learning). Metoda ν-SVM gwarantuje, że parametr ν jest dokładną frakcją punktów treningowych, które staną się wektorami nośnymi, niezależnie od rozkładu danych.|False|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z jądrem RBF, gdy gamma dąży do nieskończoności, a C jest stałe, model zawsze staje się niedouczony, niezależnie od rozkładu danych.|False|
(An Introduction to Statistical Learning). W SVM z jądrem RBF, gdy gamma dąży do nieskończoności, a C jest stałe, model zawsze staje się równoważny klasyfikatorowi najbliższego sąsiada.|True|
(The Elements of Statistical Learning). Metoda ν-SVM gwarantuje, że parametr ν jest dolnym ograniczeniem frakcji błędów treningowych i górnym ograniczeniem frakcji wektorów nośnych.|False|
(The Elements of Statistical Learning). W SVM z liniowym jądrem, wektor wag w przestrzeni cech jest zawsze kombinacją liniową wszystkich punktów treningowych, nie tylko wektorów nośnych.|False|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z miękkim marginesem, punkty treningowe z αi = 0 zawsze leżą po właściwej stronie marginesu i mają zerowe zmienne luzu (slack variables).|True|
(An Introduction to Statistical Learning). Metoda rdzeniowa w SVM (kernel trick) może być stosowana do dowolnej funkcji jądrowej, pod warunkiem, że spełnia ona twierdzenie Mercera.|True|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z jądrem RBF, zwiększenie parametru gamma zawsze prowadzi do zwiększenia złożoności modelu, niezależnie od wartości parametru C.|False|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z jądrem RBF (radial basis function), zwiększenie parametru gamma zawsze prowadzi do zmniejszenia błędu treningowego.|False|
(An Introduction to Statistical Learning). W SVM do regresji (SVR), wszystkie punkty treningowe leżące poza ε-rurą mają mnożniki Lagrange'a równe C.|True|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z jądrem liniowym, wektor normalny hiperpłaszczyzny decyzyjnej jest zawsze kombinacją liniową wszystkich punktów treningowych, nie tylko wektorów nośnych.|False|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z jądrem liniowym, zwiększanie parametru C zawsze prowadzi do zmniejszenia normy wektora wag w przestrzeni cech.|False|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM do regresji (SVR), szerokość ε-rury jest zawsze odwrotnie proporcjonalna do parametru C, niezależnie od charakterystyki danych.|False|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z jądrem wielomianowym, zwiększanie stopnia wielomianu zawsze prowadzi do zmniejszenia błędu treningowego, ale nie zawsze do zmniejszenia błędu testowego.|True|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z jądrem liniowym, zwiększanie parametru C zawsze prowadzi do zmniejszenia marginesu decyzyjnego.|False|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z miękkim marginesem, suma wszystkich mnożników Lagrange'a jest zawsze równa parametrowi C pomnożonemu przez liczbę klas.|False|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z jądrem liniowym, wektor normalny hiperpłaszczyzny decyzyjnej jest zawsze kombinacją liniową tylko tych wektorów nośnych, które leżą dokładnie na marginesie lub wewnątrz niego.|True|
(The Elements of Statistical Learning). W SVM z jądrem wielomianowym, zwiększanie stopnia wielomianu zawsze prowadzi do zmniejszenia błędu generalizacji.|False|
(An Introduction to Statistical Learning). W SVM do regresji (SVR), suma wszystkich dodatnich mnożników Lagrange'a jest zawsze równa sumie wszystkich ujemnych mnożników Lagrange'a.|True|
(The Elements of Statistical Learning). W SVM z jądrem liniowym, wektor wag w przestrzeni cech jest zawsze kombinacją liniową wektorów nośnych, nawet gdy używamy regularyzacji L1.|False|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM do regresji (SVR), wszystkie punkty treningowe leżące dokładnie na granicy ε-rury mają mnożniki Lagrange'a równe dokładnie połowie wartości parametru C.|False|
(An Introduction to Statistical Learning). W SVM z jądrem wielomianowym stopnia d, liczba cech w przestrzeni transformowanej jest zawsze równa sumie liczby kombinacji bez powtórzeń cech oryginalnych od 1 do d.|False|
(An Introduction to Statistical Learning). W SVM z jądrem RBF, gdy gamma dąży do zera, model staje się równoważny regresji logistycznej z regularyzacją L2.|False|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W przypadku SVM z miękkim marginesem, wszystkie punkty treningowe leżące wewnątrz marginesu muszą mieć niezerowe zmienne luzu (slack variables).|False|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z miękkim marginesem, suma mnożników Lagrange'a dla wszystkich punktów treningowych jest zawsze równa parametrowi C pomnożonemu przez liczbę klas.|False|
(The Elements of Statistical Learning). W SVM z jądrem RBF, gdy gamma dąży do zera, a C dąży do zera, model zawsze staje się równoważny klasyfikatorowi stałemu, który przypisuje wszystkim punktom klasę mniejszościową.|False|
(An Introduction to Statistical Learning). W SVM z liniowym jądrem, wektor wag w przestrzeni cech jest zawsze ortogonalny do wektorów nośnych leżących dokładnie na marginesie.|False|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z miękkim marginesem, punkty treningowe z 0 < αi < C zawsze mają zmienne luzu (slack variables) równe zero.|False|
(The Elements of Statistical Learning). W SVM z jądrem RBF, gdy gamma dąży do zera, a C jest stałe, model zawsze staje się równoważny klasyfikatorowi liniowemu z regularyzacją L2, którego siła regularyzacji zależy od C.|True|
(The Elements of Statistical Learning). Metoda ν-SVM gwarantuje, że parametr ν jest górnym ograniczeniem frakcji błędów treningowych i dolnym ograniczeniem frakcji wektorów nośnych.|True|
(An Introduction to Statistical Learning). W SVM z jądrem wielomianowym, zwiększanie stopnia wielomianu zawsze prowadzi do zwiększenia liczby wektorów nośnych, niezależnie od wartości parametru C.|False|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z jądrem wielomianowym, zwiększanie stopnia wielomianu zawsze prowadzi do zwiększenia liczby wektorów nośnych.|False|
(An Introduction to Statistical Learning). W SVM z jądrem wielomianowym stopnia d, liczba cech w przestrzeni transformowanej jest zawsze równa sumie liczby kombinacji bez powtórzeń od 1 do d elementów z p elementów, gdzie p jest liczbą cech w oryginalnej przestrzeni.|True|
(An Introduction to Statistical Learning). W SVM z jądrem wielomianowym, liczba wymiarów przestrzeni cech jest zawsze równa liczbie kombinacji z powtórzeniami cech oryginalnych do stopnia wielomianu.|False|
(The Elements of Statistical Learning). W SVM z jądrem RBF, gdy gamma dąży do zera, model staje się równoważny klasyfikatorowi liniowemu z regularyzacją L2.|True|
"(An Introduction to Statistical Learning). W SVM do klasyfikacji wieloklasowej metodą ""jeden-przeciwko-wszystkim"", każdy punkt treningowy jest używany jako przykład pozytywny dokładnie raz i jako przykład negatywny we wszystkich pozostałych klasyfikatorach binarnych."|True|
(The Elements of Statistical Learning). Metoda SMO (Sequential Minimal Optimization) w SVM gwarantuje, że w każdej iteracji dokładnie dwa mnożniki Lagrange'a zmieniają swoje wartości.|True|
(The Elements of Statistical Learning). Metoda SMO (Sequential Minimal Optimization) w SVM zawsze wybiera do optymalizacji parę mnożników Lagrange'a, które najbardziej naruszają warunki KKT.|False|
(An Introduction to Statistical Learning). W SVM z jądrem RBF, gdy gamma dąży do nieskończoności, a C dąży do zera, model zawsze staje się równoważny klasyfikatorowi najbliższego sąsiada z głosowaniem ważonym odwrotnością odległości.|False|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z miękkim marginesem, suma mnożników Lagrange'a dla punktów treningowych z klasy pozytywnej jest zawsze równa sumie mnożników Lagrange'a dla punktów z klasy negatywnej.|True|
(An Introduction to Statistical Learning). W SVM z miękkim marginesem, punkty treningowe z αi = 0 zawsze leżą po właściwej stronie marginesu i są poprawnie sklasyfikowane.|True|
(The Elements of Statistical Learning). Metoda ν-SVM gwarantuje, że liczba wektorów nośnych jest zawsze równa ν razy liczba przykładów treningowych, niezależnie od rozkładu danych.|False|
(The Elements of Statistical Learning). W SVM z jądrem wielomianowym, zwiększanie stopnia wielomianu zawsze prowadzi do zmniejszenia błędu treningowego, niezależnie od wartości parametru C.|False|
(The Elements of Statistical Learning). Jądro wielomianowe w SVM gwarantuje, że dane będą liniowo separowalne w przestrzeni cech, niezależnie od stopnia wielomianu.|False|
(The Elements of Statistical Learning). Metoda SMO (Sequential Minimal Optimization) w SVM gwarantuje, że w każdej iteracji suma iloczynów mnożników Lagrange'a i etykiet klas pozostaje stała.|True|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z miękkim marginesem, punkty treningowe z 0 < αi < C zawsze leżą dokładnie na marginesie decyzyjnym.|True|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). One-class SVM może być skutecznie stosowany do detekcji anomalii, ale wymaga zbioru treningowego zawierającego zarówno przykłady normalne, jak i anomalie.|False|
(An Introduction to Statistical Learning). Metoda SMO (Sequential Minimal Optimization) stosowana w SVM zawsze zbiegnie się do globalnego minimum funkcji celu, niezależnie od wyboru początkowych mnożników Lagrange'a.|False|
(The Elements of Statistical Learning). W SVM z liniowym jądrem, wektor wag w przestrzeni cech jest zawsze prostopadły do hiperpłaszczyzny rozdzielającej klasy w punkcie najbliższym początku układu współrzędnych.|True|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z jądrem liniowym, wektor normalny hiperpłaszczyzny decyzyjnej jest zawsze kombinacją liniową tylko tych wektorów nośnych, które leżą dokładnie na marginesie.|False|
(An Introduction to Statistical Learning). W SVM z jądrem wielomianowym stopnia d, liczba cech w przestrzeni transformowanej jest zawsze równa d do potęgi liczby cech w oryginalnej przestrzeni.|False|
(The Elements of Statistical Learning). W SVM z jądrem RBF, gdy gamma dąży do nieskończoności, a C jest stałe, model zawsze staje się równoważny klasyfikatorowi najbliższego sąsiada z odległością cosinusową.|False|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z miękkim marginesem, suma wszystkich zmiennych luzu (slack variables) jest zawsze równa parametrowi C.|False|
(An Introduction to Statistical Learning). W SVM z jądrem RBF, gdy gamma dąży do zera, model staje się równoważny klasyfikatorowi stałemu, który zawsze przewiduje klasę większościową.|False|
(An Introduction to Statistical Learning). W SVM z jądrem wielomianowym stopnia d, liczba cech w przestrzeni transformowanej jest zawsze równa liczbie kombinacji z powtórzeniami d+p elementów, gdzie p jest liczbą cech w oryginalnej przestrzeni.|True|
(An Introduction to Statistical Learning). W SVM z jądrem RBF, gdy gamma dąży do zera, model staje się równoważny klasyfikatorowi liniowemu bez regularyzacji.|False|
(The Elements of Statistical Learning). W SVM z liniowym jądrem, wektor wag w przestrzeni cech jest zawsze kombinacją liniową tylko tych wektorów nośnych, które mają niezerowe zmienne luzu (slack variables).|False|
(The Elements of Statistical Learning). Metoda ν-SVM gwarantuje, że parametr ν jest dokładną frakcją punktów treningowych, które staną się wektorami nośnymi lub będą błędnie sklasyfikowane, niezależnie od rozkładu danych.|False|
(The Elements of Statistical Learning). Metoda SMO (Sequential Minimal Optimization) w SVM gwarantuje, że w każdej iteracji co najmniej jeden z dwóch wybranych mnożników Lagrange'a zmieni swoją wartość.|True|
(The Elements of Statistical Learning). W SVM z jądrem wielomianowym, zwiększanie stopnia wielomianu zawsze prowadzi do zwiększenia efektywnej liczby wymiarów w przestrzeni cech.|True|
(An Introduction to Statistical Learning). W SVM do regresji (SVR), wszystkie punkty treningowe leżące wewnątrz ε-rury mają zerowe mnożniki Lagrange'a.|True|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM do regresji (SVR), szerokość ε-rury jest zawsze odwrotnie proporcjonalna do pierwiastka z parametru C, niezależnie od charakterystyki danych.|False|
