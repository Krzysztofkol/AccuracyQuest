_accuracy-quest/
├── questions.csv (21975 bytes)
├── accuracy-quest-frontend.html (9371 bytes)
├── accuracy-quest-backend.py (3298 bytes)
└── start-accuracy-quest.bat (90 bytes)

### `questions.csv` file (21975 bytes):

```
question|answer|user_answer
(The Elements of Statistical Learning). W SVM z jądrem RBF, gdy gamma dąży do nieskończoności, a C dąży do zera, model zawsze staje się równoważny klasyfikatorowi najbliższego sąsiada z głosowaniem ważonym odległością.|False|
(The Elements of Statistical Learning). W SVM z liniowym jądrem, wektor wag w przestrzeni cech jest zawsze prostopadły do hiperpłaszczyzny rozdzielającej klasy w punkcie środkowym między najbliższymi punktami przeciwnych klas.|False|
(An Introduction to Statistical Learning). Warunek KKT (Karush-Kuhn-Tucker) mówiący, że α_i * (y_i(w^T x_i + b) - 1 + ξ_i) = 0, implikuje, że wszystkie wektory nośne leżą dokładnie na marginesie w przypadku miękkiego SVM.|False|
(An Introduction to Statistical Learning). W SVM do klasyfikacji wieloklasowej metodą "jeden-przeciwko-jednemu", liczba trenowanych klasyfikatorów jest zawsze równa sumie liczb naturalnych od 1 do (n-1), gdzie n to liczba klas.|True|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z liniowym jądrem, wektor normalny hiperpłaszczyzny decyzyjnej jest zawsze ortogonalny do wszystkich wektorów nośnych.|False|
(The Elements of Statistical Learning). Metoda SMO (Sequential Minimal Optimization) w SVM gwarantuje, że w każdej iteracji suma mnożników Lagrange'a pozostaje stała.|True|
(An Introduction to Statistical Learning). W SVM z miękkim marginesem, zwiększenie parametru C zawsze prowadzi do zmniejszenia liczby wektorów nośnych.|False|
(An Introduction to Statistical Learning). W SVM do klasyfikacji wieloklasowej metodą "jeden-przeciwko-jednemu", każdy punkt treningowy jest używany jako przykład pozytywny dokładnie w tylu klasyfikatorach binarnych, ile jest pozostałych klas.|True|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z jądrem RBF, gdy gamma dąży do nieskończoności, a C jest stałe, model zawsze staje się przeuczony, niezależnie od rozkładu danych.|False|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z miękkim marginesem, suma wszystkich zmiennych luzu (slack variables) jest zawsze mniejsza lub równa parametrowi C pomnożonemu przez liczbę przykładów treningowych.|True|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM do klasyfikacji wieloklasowej metodą "jeden-przeciwko-wszystkim", liczba trenowanych klasyfikatorów jest zawsze równa liczbie klas minus jeden.|False|
(An Introduction to Statistical Learning). W SVM do regresji (SVR), punkty treningowe leżące na granicy ε-rury zawsze mają niezerowe mnożniki Lagrange'a.|True|
(An Introduction to Statistical Learning). W SVM z jądrem wielomianowym stopnia d, liczba cech w przestrzeni transformowanej jest zawsze równa liczbie permutacji z powtórzeniami d elementów z p elementów, gdzie p jest liczbą cech w oryginalnej przestrzeni.|False|
(An Introduction to Statistical Learning). W SVM z jądrem RBF, gdy gamma dąży do nieskończoności, a C jest stałe, model zawsze staje się równoważny klasyfikatorowi najbliższego sąsiada z odległością Manhattan.|False|
(The Elements of Statistical Learning). Metoda ν-SVM gwarantuje, że parametr ν jest górnym ograniczeniem frakcji błędów treningowych i dolnym ograniczeniem frakcji wektorów nośnych tylko dla pewnych wartości ν z przedziału (0,1).|False|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z jądrem wielomianowym, zwiększanie stopnia wielomianu zawsze prowadzi do zwiększenia liczby wektorów nośnych, ale nie zawsze do poprawy separacji klas w oryginalnej przestrzeni cech.|True|
(The Elements of Statistical Learning). Metoda ν-SVM gwarantuje, że parametr ν jest dokładną frakcją wektorów nośnych w zbiorze treningowym, niezależnie od rozkładu danych.|False|
(The Elements of Statistical Learning). W SVM z miękkim marginesem, punkty treningowe z αi = C są zawsze błędnie sklasyfikowane przez model.|False|
(An Introduction to Statistical Learning). W SVM do regresji (SVR), punkty treningowe leżące wewnątrz ε-rury zawsze mają dodatnie mnożniki Lagrange'a, ale mniejsze niż C.|False|
(The Elements of Statistical Learning). Metoda SMO (Sequential Minimal Optimization) w SVM gwarantuje, że w każdej iteracji co najmniej jeden mnożnik Lagrange'a zmieni swoją wartość.|False|
(The Elements of Statistical Learning). W SVM z jądrem RBF, gdy gamma dąży do nieskończoności, a C dąży do nieskończoności, model zawsze staje się równoważny klasyfikatorowi najbliższego sąsiada z odległością euklidesową.|True|
(An Introduction to Statistical Learning). W SVM z jądrem wielomianowym stopnia d, liczba cech w przestrzeni transformowanej jest zawsze równa d do potęgi liczby cech w oryginalnej przestrzeni plus jeden.|False|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z jądrem liniowym, zwiększanie parametru C zawsze prowadzi do zwiększenia liczby wektorów nośnych.|False|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z miękkim marginesem, punkty treningowe z 0 < αi < C zawsze leżą dokładnie na marginesie decyzyjnym i mają zerowe zmienne luzu (slack variables).|False|
(An Introduction to Statistical Learning). W SVM z jądrem RBF, gdy gamma dąży do zera, a C dąży do zera, model zawsze staje się równoważny klasyfikatorowi stałemu, który przypisuje wszystkim punktom średnią ważoną klas treningowych.|False|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z jądrem wielomianowym, zwiększanie stopnia wielomianu zawsze prowadzi do zmniejszenia błędu generalizacji, niezależnie od wartości parametru C.|False|
(An Introduction to Statistical Learning). W SVM z jądrem RBF, gdy gamma dąży do zera, a C dąży do nieskończoności, model zawsze staje się równoważny klasyfikatorowi liniowemu bez regularyzacji.|True|
(An Introduction to Statistical Learning). W SVM do regresji (SVR), punkty treningowe leżące dokładnie na granicy ε-rury zawsze mają mnożniki Lagrange'a równe połowie wartości parametru C.|False|
(An Introduction to Statistical Learning). W SVM z jądrem wielomianowym, zwiększanie stopnia wielomianu zawsze prowadzi do zmniejszenia marginesu funkcjonalnego w przestrzeni cech, niezależnie od wartości parametru C.|False|
(The Elements of Statistical Learning). W SVM z jądrem RBF, gdy gamma dąży do zera, a C jest stałe, model zawsze staje się równoważny klasyfikatorowi stałemu, który przypisuje wszystkim punktom klasę większościową.|True|
(The Elements of Statistical Learning). W SVM z jądrem RBF, gdy gamma dąży do nieskończoności, a C dąży do nieskończoności, model zawsze staje się równoważny klasyfikatorowi najbliższego sąsiada z odległością Mahalanobisa.|False|
(An Introduction to Statistical Learning). W SVM do klasyfikacji wieloklasowej metodą "jeden-przeciwko-jednemu", każdy punkt treningowy jest używany jako przykład negatywny dokładnie w tylu klasyfikatorach binarnych, ile jest klas minus dwa.|True|
(The Elements of Statistical Learning). W SVM do klasyfikacji wieloklasowej metodą "jeden-przeciwko-jednemu", liczba trenowanych klasyfikatorów jest zawsze równa n(n-1)/2, gdzie n to liczba klas.|True|
(The Elements of Statistical Learning). W SVM z jądrem gaussowskim, gdy gamma dąży do nieskończoności, klasyfikator SVM zawsze staje się równoważny klasyfikatorowi k-najbliższych sąsiadów z k=1.|False|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM do regresji (SVR), wszystkie punkty treningowe leżące dokładnie na granicy ε-rury mają niezerowe mnożniki Lagrange'a, ale mniejsze niż C.|True|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM do regresji (SVR), wszystkie punkty treningowe leżące poza ε-rurą mają zmienne luzu (slack variables) większe od zera, ale mniejsze od ε.|False|
(An Introduction to Statistical Learning). W SVM z jądrem wielomianowym, zwiększanie stopnia wielomianu zawsze prowadzi do zwiększenia efektywnej liczby wymiarów w przestrzeni cech, ale nie zawsze do poprawy separacji klas.|True|
(The Elements of Statistical Learning). W SVM z liniowym jądrem, wektor wag w przestrzeni cech jest zawsze ortogonalny do różnicy dowolnych dwóch wektorów nośnych z przeciwnych klas.|True|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z miękkim marginesem, punkty treningowe z αi = C zawsze leżą po niewłaściwej stronie marginesu lub wewnątrz niego.|True|
(The Elements of Statistical Learning). Metoda ν-SVM gwarantuje, że parametr ν jest dokładną frakcją punktów treningowych, które staną się wektorami nośnymi, niezależnie od rozkładu danych.|False|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z jądrem RBF, gdy gamma dąży do nieskończoności, a C jest stałe, model zawsze staje się niedouczony, niezależnie od rozkładu danych.|False|
(An Introduction to Statistical Learning). W SVM z jądrem RBF, gdy gamma dąży do nieskończoności, a C jest stałe, model zawsze staje się równoważny klasyfikatorowi najbliższego sąsiada.|True|
(The Elements of Statistical Learning). Metoda ν-SVM gwarantuje, że parametr ν jest dolnym ograniczeniem frakcji błędów treningowych i górnym ograniczeniem frakcji wektorów nośnych.|False|
(The Elements of Statistical Learning). W SVM z liniowym jądrem, wektor wag w przestrzeni cech jest zawsze kombinacją liniową wszystkich punktów treningowych, nie tylko wektorów nośnych.|False|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z miękkim marginesem, punkty treningowe z αi = 0 zawsze leżą po właściwej stronie marginesu i mają zerowe zmienne luzu (slack variables).|True|
(An Introduction to Statistical Learning). Metoda rdzeniowa w SVM (kernel trick) może być stosowana do dowolnej funkcji jądrowej, pod warunkiem, że spełnia ona twierdzenie Mercera.|True|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z jądrem RBF, zwiększenie parametru gamma zawsze prowadzi do zwiększenia złożoności modelu, niezależnie od wartości parametru C.|False|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z jądrem RBF (radial basis function), zwiększenie parametru gamma zawsze prowadzi do zmniejszenia błędu treningowego.|False|
(An Introduction to Statistical Learning). W SVM do regresji (SVR), wszystkie punkty treningowe leżące poza ε-rurą mają mnożniki Lagrange'a równe C.|True|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z jądrem liniowym, wektor normalny hiperpłaszczyzny decyzyjnej jest zawsze kombinacją liniową wszystkich punktów treningowych, nie tylko wektorów nośnych.|False|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z jądrem liniowym, zwiększanie parametru C zawsze prowadzi do zmniejszenia normy wektora wag w przestrzeni cech.|False|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM do regresji (SVR), szerokość ε-rury jest zawsze odwrotnie proporcjonalna do parametru C, niezależnie od charakterystyki danych.|False|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z jądrem wielomianowym, zwiększanie stopnia wielomianu zawsze prowadzi do zmniejszenia błędu treningowego, ale nie zawsze do zmniejszenia błędu testowego.|True|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z jądrem liniowym, zwiększanie parametru C zawsze prowadzi do zmniejszenia marginesu decyzyjnego.|False|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z miękkim marginesem, suma wszystkich mnożników Lagrange'a jest zawsze równa parametrowi C pomnożonemu przez liczbę klas.|False|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z jądrem liniowym, wektor normalny hiperpłaszczyzny decyzyjnej jest zawsze kombinacją liniową tylko tych wektorów nośnych, które leżą dokładnie na marginesie lub wewnątrz niego.|True|
(The Elements of Statistical Learning). W SVM z jądrem wielomianowym, zwiększanie stopnia wielomianu zawsze prowadzi do zmniejszenia błędu generalizacji.|False|
(An Introduction to Statistical Learning). W SVM do regresji (SVR), suma wszystkich dodatnich mnożników Lagrange'a jest zawsze równa sumie wszystkich ujemnych mnożników Lagrange'a.|True|
(The Elements of Statistical Learning). W SVM z jądrem liniowym, wektor wag w przestrzeni cech jest zawsze kombinacją liniową wektorów nośnych, nawet gdy używamy regularyzacji L1.|False|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM do regresji (SVR), wszystkie punkty treningowe leżące dokładnie na granicy ε-rury mają mnożniki Lagrange'a równe dokładnie połowie wartości parametru C.|False|
(An Introduction to Statistical Learning). W SVM z jądrem wielomianowym stopnia d, liczba cech w przestrzeni transformowanej jest zawsze równa sumie liczby kombinacji bez powtórzeń cech oryginalnych od 1 do d.|False|
(An Introduction to Statistical Learning). W SVM z jądrem RBF, gdy gamma dąży do zera, model staje się równoważny regresji logistycznej z regularyzacją L2.|False|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W przypadku SVM z miękkim marginesem, wszystkie punkty treningowe leżące wewnątrz marginesu muszą mieć niezerowe zmienne luzu (slack variables).|False|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z miękkim marginesem, suma mnożników Lagrange'a dla wszystkich punktów treningowych jest zawsze równa parametrowi C pomnożonemu przez liczbę klas.|False|
(The Elements of Statistical Learning). W SVM z jądrem RBF, gdy gamma dąży do zera, a C dąży do zera, model zawsze staje się równoważny klasyfikatorowi stałemu, który przypisuje wszystkim punktom klasę mniejszościową.|False|
(An Introduction to Statistical Learning). W SVM z liniowym jądrem, wektor wag w przestrzeni cech jest zawsze ortogonalny do wektorów nośnych leżących dokładnie na marginesie.|False|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z miękkim marginesem, punkty treningowe z 0 < αi < C zawsze mają zmienne luzu (slack variables) równe zero.|False|
(The Elements of Statistical Learning). W SVM z jądrem RBF, gdy gamma dąży do zera, a C jest stałe, model zawsze staje się równoważny klasyfikatorowi liniowemu z regularyzacją L2, którego siła regularyzacji zależy od C.|True|
(The Elements of Statistical Learning). Metoda ν-SVM gwarantuje, że parametr ν jest górnym ograniczeniem frakcji błędów treningowych i dolnym ograniczeniem frakcji wektorów nośnych.|True|
(An Introduction to Statistical Learning). W SVM z jądrem wielomianowym, zwiększanie stopnia wielomianu zawsze prowadzi do zwiększenia liczby wektorów nośnych, niezależnie od wartości parametru C.|False|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z jądrem wielomianowym, zwiększanie stopnia wielomianu zawsze prowadzi do zwiększenia liczby wektorów nośnych.|False|
(An Introduction to Statistical Learning). W SVM z jądrem wielomianowym stopnia d, liczba cech w przestrzeni transformowanej jest zawsze równa sumie liczby kombinacji bez powtórzeń od 1 do d elementów z p elementów, gdzie p jest liczbą cech w oryginalnej przestrzeni.|True|
(An Introduction to Statistical Learning). W SVM z jądrem wielomianowym, liczba wymiarów przestrzeni cech jest zawsze równa liczbie kombinacji z powtórzeniami cech oryginalnych do stopnia wielomianu.|False|
(The Elements of Statistical Learning). W SVM z jądrem RBF, gdy gamma dąży do zera, model staje się równoważny klasyfikatorowi liniowemu z regularyzacją L2.|True|
(An Introduction to Statistical Learning). W SVM do klasyfikacji wieloklasowej metodą "jeden-przeciwko-wszystkim", każdy punkt treningowy jest używany jako przykład pozytywny dokładnie raz i jako przykład negatywny we wszystkich pozostałych klasyfikatorach binarnych.|True|
(The Elements of Statistical Learning). Metoda SMO (Sequential Minimal Optimization) w SVM gwarantuje, że w każdej iteracji dokładnie dwa mnożniki Lagrange'a zmieniają swoje wartości.|True|
(The Elements of Statistical Learning). Metoda SMO (Sequential Minimal Optimization) w SVM zawsze wybiera do optymalizacji parę mnożników Lagrange'a, które najbardziej naruszają warunki KKT.|False|
(An Introduction to Statistical Learning). W SVM z jądrem RBF, gdy gamma dąży do nieskończoności, a C dąży do zera, model zawsze staje się równoważny klasyfikatorowi najbliższego sąsiada z głosowaniem ważonym odwrotnością odległości.|False|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z miękkim marginesem, suma mnożników Lagrange'a dla punktów treningowych z klasy pozytywnej jest zawsze równa sumie mnożników Lagrange'a dla punktów z klasy negatywnej.|True|
(An Introduction to Statistical Learning). W SVM z miękkim marginesem, punkty treningowe z αi = 0 zawsze leżą po właściwej stronie marginesu i są poprawnie sklasyfikowane.|True|
(The Elements of Statistical Learning). Metoda ν-SVM gwarantuje, że liczba wektorów nośnych jest zawsze równa ν razy liczba przykładów treningowych, niezależnie od rozkładu danych.|False|
(The Elements of Statistical Learning). W SVM z jądrem wielomianowym, zwiększanie stopnia wielomianu zawsze prowadzi do zmniejszenia błędu treningowego, niezależnie od wartości parametru C.|False|
(The Elements of Statistical Learning). Jądro wielomianowe w SVM gwarantuje, że dane będą liniowo separowalne w przestrzeni cech, niezależnie od stopnia wielomianu.|False|
(The Elements of Statistical Learning). Metoda SMO (Sequential Minimal Optimization) w SVM gwarantuje, że w każdej iteracji suma iloczynów mnożników Lagrange'a i etykiet klas pozostaje stała.|True|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z miękkim marginesem, punkty treningowe z 0 < αi < C zawsze leżą dokładnie na marginesie decyzyjnym.|True|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). One-class SVM może być skutecznie stosowany do detekcji anomalii, ale wymaga zbioru treningowego zawierającego zarówno przykłady normalne, jak i anomalie.|False|
(An Introduction to Statistical Learning). Metoda SMO (Sequential Minimal Optimization) stosowana w SVM zawsze zbiegnie się do globalnego minimum funkcji celu, niezależnie od wyboru początkowych mnożników Lagrange'a.|False|
(The Elements of Statistical Learning). W SVM z liniowym jądrem, wektor wag w przestrzeni cech jest zawsze prostopadły do hiperpłaszczyzny rozdzielającej klasy w punkcie najbliższym początku układu współrzędnych.|True|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z jądrem liniowym, wektor normalny hiperpłaszczyzny decyzyjnej jest zawsze kombinacją liniową tylko tych wektorów nośnych, które leżą dokładnie na marginesie.|False|
(An Introduction to Statistical Learning). W SVM z jądrem wielomianowym stopnia d, liczba cech w przestrzeni transformowanej jest zawsze równa d do potęgi liczby cech w oryginalnej przestrzeni.|False|
(The Elements of Statistical Learning). W SVM z jądrem RBF, gdy gamma dąży do nieskończoności, a C jest stałe, model zawsze staje się równoważny klasyfikatorowi najbliższego sąsiada z odległością cosinusową.|False|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM z miękkim marginesem, suma wszystkich zmiennych luzu (slack variables) jest zawsze równa parametrowi C.|False|
(An Introduction to Statistical Learning). W SVM z jądrem RBF, gdy gamma dąży do zera, model staje się równoważny klasyfikatorowi stałemu, który zawsze przewiduje klasę większościową.|False|
(An Introduction to Statistical Learning). W SVM z jądrem wielomianowym stopnia d, liczba cech w przestrzeni transformowanej jest zawsze równa liczbie kombinacji z powtórzeniami d+p elementów, gdzie p jest liczbą cech w oryginalnej przestrzeni.|True|
(An Introduction to Statistical Learning). W SVM z jądrem RBF, gdy gamma dąży do zera, model staje się równoważny klasyfikatorowi liniowemu bez regularyzacji.|False|
(The Elements of Statistical Learning). W SVM z liniowym jądrem, wektor wag w przestrzeni cech jest zawsze kombinacją liniową tylko tych wektorów nośnych, które mają niezerowe zmienne luzu (slack variables).|False|
(The Elements of Statistical Learning). Metoda ν-SVM gwarantuje, że parametr ν jest dokładną frakcją punktów treningowych, które staną się wektorami nośnymi lub będą błędnie sklasyfikowane, niezależnie od rozkładu danych.|False|
(The Elements of Statistical Learning). Metoda SMO (Sequential Minimal Optimization) w SVM gwarantuje, że w każdej iteracji co najmniej jeden z dwóch wybranych mnożników Lagrange'a zmieni swoją wartość.|True|
(The Elements of Statistical Learning). W SVM z jądrem wielomianowym, zwiększanie stopnia wielomianu zawsze prowadzi do zwiększenia efektywnej liczby wymiarów w przestrzeni cech.|True|
(An Introduction to Statistical Learning). W SVM do regresji (SVR), wszystkie punkty treningowe leżące wewnątrz ε-rury mają zerowe mnożniki Lagrange'a.|True|
(Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow). W SVM do regresji (SVR), szerokość ε-rury jest zawsze odwrotnie proporcjonalna do pierwiastka z parametru C, niezależnie od charakterystyki danych.|False|
```

### `accuracy-quest-frontend.html` file (9371 bytes):

```
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AccuracyQuest</title>
    <style>
        body {
            background-color: #f3f4f6;
            font-family: Arial, sans-serif;
        }
        .container {
            max-width: 600px;
            margin: 50px auto;
            padding: 20px;
            background-color: #ffffff;
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        }
        .header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 20px;
        }
        .accuracy {
            font-size: 18px;
            font-weight: bold;
        }
        .question-info {
            font-size: 18px;
            font-weight: bold;
            display: flex;
            align-items: center;
        }
        #currentQuestion {
            width: 30px;
            text-align: center;
            margin: 0 5px;
        }
        .question {
            font-size: 24px;
            margin-top: 20px;
            margin-bottom: 20px;
            text-align: center;
        }
        .buttons-container {
            display: flex;
            flex-direction: column;
            gap: 10px;
            margin-bottom: 20px; /* Add margin-bottom to buttons-container */
        }
        .button-row {
            display: flex;
            gap: 10px;
        }
        .button {
            flex: 1;
            padding: 15px 20px;
            border: none;
            border-radius: 4px;
            cursor: pointer;
            font-size: 16px;
            text-align: center;
        }
        .button:disabled {
            cursor: not-allowed;
        }
        .navigation-button {
            background-color: #4299e1;
            color: white;
        }
        .navigation-button:disabled {
            background-color: #a0aec0;
        }
        .answer-button {
            background-color: #e2e8f0;
        }
        .correct {
            background-color: #48bb78;
            color: white;
        }
        .incorrect {
            background-color: #e53e3e;
            color: white;
        }
    </style>
</head>
<body>
<div id="app" class="container">
    <div class="header">
        <div>
            <div class="accuracy">
                Total Accuracy: <span id="totalAccuracy">0</span>%
            </div>
            <div class="accuracy">
                20-Rolling Accuracy: <span id="rollingAccuracy">0</span>%
            </div>
        </div>
        <div class="question-info">
            Question <input type="text" id="currentQuestion" value="1">/<span id="totalQuestions">0</span>
        </div>
    </div>
    <div class="buttons-container">
        <div class="button-row">
            <button onclick="navigate(-1)" id="backButton" class="button navigation-button">Back</button>
            <button onclick="navigate(1)" id="forwardButton" class="button navigation-button">Forward</button>
        </div>
        <div class="button-row">
            <button onclick="submitAnswer('True')" id="trueButton" class="button answer-button">True</button>
            <button onclick="submitAnswer('False')" id="falseButton" class="button answer-button">False</button>
        </div>
    </div>
    <div id="questionContainer">
        <p id="questionText" class="question"></p>
    </div>
</div>
<script>
let questions = [];
let currentIndex = 0;
let totalCorrect = 0;
let answeredQuestions = [];
async function fetchWithRetry(url, options = {}, retries = 3) {
    try {
        const response = await fetch(url, options);
        if (!response.ok) {
            throw new Error(`HTTP error! status: ${response.status}`);
        }
        return response;
    } catch (error) {
        if (retries > 0) {
            console.log(`Retrying fetch to ${url}. Attempts left: ${retries - 1}`);
            await new Promise(resolve => setTimeout(resolve, 1000));
            return fetchWithRetry(url, options, retries - 1);
        }
        throw error;
    }
}
async function checkBackendHealth() {
    try {
        const response = await fetchWithRetry('/health');
        const data = await response.json();
        if (data.status === 'healthy') {
            await fetchQuestions();
        }
    } catch (error) {
        console.error("Backend health check failed:", error);
        document.getElementById('questionText').textContent = `Error: Unable to connect to the backend. Please check if the server is running. (${error.message})`;
    }
}
async function fetchQuestions() {
    try {
        const response = await fetchWithRetry('/api/questions');
        questions = await response.json();
        if (Array.isArray(questions) && questions.length > 0) {
            document.getElementById('totalQuestions').textContent = questions.length;
            initializeAnsweredQuestions();
            calculateAccuracies();
            displayQuestion();
        } else {
            throw new Error("No questions received from the server");
        }
    } catch (error) {
        console.error("Error fetching questions:", error);
        document.getElementById('questionText').textContent = `Error loading questions: ${error.message}. Please check the console and ensure the backend is running.`;
    }
}
function initializeAnsweredQuestions() {
    answeredQuestions = questions.map(q => ({ ...q, correct: q.user_answer === q.correct_answer }));
}
function calculateAccuracies() {
    const answeredCount = answeredQuestions.filter(q => q.user_answer !== '').length;
    totalCorrect = answeredQuestions.filter(q => q.user_answer === q.correct_answer).length;
    const totalAccuracy = (answeredCount === 0) ? 0 : (totalCorrect / answeredCount) * 100;
    const last20Answered = answeredQuestions.filter(q => q.user_answer !== '').slice(-20);
    const last20Correct = last20Answered.filter(q => q.user_answer === q.correct_answer).length;
    const rollingAccuracy = (last20Answered.length === 0) ? 0 : (last20Correct / last20Answered.length) * 100;
    
    document.getElementById('totalAccuracy').textContent = totalAccuracy.toFixed(0);
    document.getElementById('rollingAccuracy').textContent = rollingAccuracy.toFixed(0);
}
function displayQuestion() {
    if (questions.length === 0) {
        document.getElementById('questionText').textContent = 'No questions available.';
        return;
    }
    const question = questions[currentIndex];
    document.getElementById('questionText').textContent = question.question;
    document.getElementById('currentQuestion').value = currentIndex + 1;
    updateButtons();
}
function updateButtons() {
    const question = questions[currentIndex];
    if (question.user_answer) {
        document.getElementById('trueButton').disabled = true;
        document.getElementById('falseButton').disabled = true;
        colorAnswers(question.user_answer === question.correct_answer, question.user_answer);
    } else {
        document.getElementById('trueButton').disabled = false;
        document.getElementById('falseButton').disabled = false;
        document.getElementById('trueButton').className = 'button answer-button';
        document.getElementById('falseButton').className = 'button answer-button';
    }
    document.getElementById('backButton').disabled = currentIndex === 0;
    document.getElementById('forwardButton').disabled = currentIndex === questions.length - 1;
}
function colorAnswers(isCorrect, answer) {
    document.getElementById('trueButton').className = 'button answer-button';
    document.getElementById('falseButton').className = 'button answer-button';
    const selectedButton = document.getElementById(`${answer.toLowerCase()}Button`);
    selectedButton.className = `button answer-button ${isCorrect ? 'correct' : 'incorrect'}`;
}
async function submitAnswer(answer) {
    try {
        const response = await fetchWithRetry('/api/answer', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
            },
            body: JSON.stringify({ index: currentIndex, answer }),
        });
        const data = await response.json();
        if (data.success) {
            const question = questions[currentIndex];
            question.user_answer = answer;
            const isCorrect = data.correct;
            answeredQuestions[currentIndex].correct = isCorrect;
            answeredQuestions[currentIndex].user_answer = answer;
            if (isCorrect) {
                totalCorrect++;
            }
            calculateAccuracies();
            colorAnswers(isCorrect, answer);
            updateButtons();
        }
    } catch (error) {
        console.error("Error submitting answer:", error);
        alert(`Error submitting answer: ${error.message}`);
    }
}
function navigate(direction) {
    currentIndex += direction;
    displayQuestion();
}
document.getElementById('currentQuestion').addEventListener('change', (event) => {
    let value = parseInt(event.target.value, 10);
    if (isNaN(value) || value < 1 || value > questions.length) {
        event.target.value = currentIndex + 1;
    } else {
        currentIndex = value - 1;
        displayQuestion();
    }
});
checkBackendHealth();
</script>
</body>
</html>
```

### `accuracy-quest-backend.py` file (3298 bytes):

```
# accuracy-quest-backend.py
import os
import csv
from flask import Flask, jsonify, request, send_from_directory
from flask_cors import CORS
import logging

app = Flask(__name__, static_folder='.')
CORS(app)  # Enable CORS for all routes

# Configure logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

CSV_FILE = 'questions.csv'

def read_questions():
    questions = []
    try:
        with open(CSV_FILE, 'r', encoding='utf-8') as file:
            reader = csv.reader(file, delimiter='|')
            next(reader)  # Skip header
            for row in reader:
                if len(row) >= 2:
                    questions.append({
                        'question': row[0],
                        'correct_answer': row[1],
                        'user_answer': row[2] if len(row) > 2 else ''
                    })
        logger.info(f"Successfully loaded {len(questions)} questions from {CSV_FILE}")
    except Exception as e:
        logger.error(f"Error reading CSV file: {e}")
    return questions

def write_questions(questions):
    try:
        with open(CSV_FILE, 'w', encoding='utf-8', newline='') as file:
            writer = csv.writer(file, delimiter='|')
            writer.writerow(['question', 'answer', 'user_answer'])
            for q in questions:
                writer.writerow([q['question'], q['correct_answer'], q['user_answer']])
        logger.info(f"Successfully wrote {len(questions)} questions to {CSV_FILE}")
    except Exception as e:
        logger.error(f"Error writing to CSV file: {e}")

@app.route('/')
def index():
    return send_from_directory('.', 'accuracy-quest-frontend.html')

@app.route('/health')
def health_check():
    logger.info("Health check endpoint accessed")
    return jsonify({"status": "healthy"}), 200

@app.route('/api/questions')
def get_questions():
    logger.info("Fetching questions")
    questions = read_questions()
    if not questions:
        logger.error("No questions available")
        return jsonify({"error": "No questions available"}), 500
    return jsonify(questions)

@app.route('/api/answer', methods=['POST'])
def submit_answer():
    data = request.json
    logger.info(f"Received answer submission: {data}")
    if not data or 'index' not in data or 'answer' not in data:
        logger.error("Invalid request data for answer submission")
        return jsonify({'success': False, 'error': 'Invalid request data'}), 400
    
    index, answer = data['index'], data['answer']
    questions = read_questions()
    if 0 <= index < len(questions):
        correct = (answer == questions[index]['correct_answer'])
        questions[index]['user_answer'] = answer
        write_questions(questions)
        logger.info(f"Answer submitted successfully for question {index}")
        return jsonify({
            'success': True,
            'correct': correct,
            'correct_answer': questions[index]['correct_answer']
        })
    
    logger.error(f"Invalid question index: {index}")
    return jsonify({'success': False, 'error': 'Invalid question index'}), 400

if __name__ == '__main__':
    logger.info(f"Current working directory: {os.getcwd()}")
    logger.info(f"CSV file path: {os.path.abspath(CSV_FILE)}")
    app.run(host='0.0.0.0', port=8080, debug=True)
```

### `start-accuracy-quest.bat` file (90 bytes):

```
@echo off
cd /d "%~dp0"
start python accuracy-quest-backend.py
start http://localhost:8080
```
